{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1DSdBL-jktGhyl5iKWdyHZCD9bWK9xu_g",
      "authorship_tag": "ABX9TyOsclD0ZIHMzW8C6lNHDEYL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashong-Nartey/Hyperspectral-Image-Classification-/blob/main/HSI_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf85K9PZUoNR",
        "outputId": "3ae18be4-1bf4-4a7d-bdb3-14de5083b174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/geniter.py /content/drive/MyDrive/HSI/\n",
        "!cp -r /content/utils.py /content/drive/MyDrive/HSI/\n",
        "!cp -r /content/record.py /content/drive/MyDrive/HSI/\n",
        "!cp -r /content/dataset/ /content/drive/MyDrive/HSI/"
      ],
      "metadata": {
        "id": "RtPGqJZnZhZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8087c58-1fa6-42ac-df98-422fc1797300"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: failed to access '/content/drive/MyDrive/HSI/': Transport endpoint is not connected\n",
            "cp: failed to access '/content/drive/MyDrive/HSI/': Transport endpoint is not connected\n",
            "cp: failed to access '/content/drive/MyDrive/HSI/': Transport endpoint is not connected\n",
            "cp: failed to access '/content/drive/MyDrive/HSI/': Transport endpoint is not connected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spectral==0.20\n",
        "!pip install torch-summary==1.2.0\n",
        "!pip install torch-optimizer==0.0.1a12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM75-GjobR01",
        "outputId": "49dc611c-dad7-4a49-ef21-12f42126a479"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spectral==0.20\n",
            "  Downloading spectral-0.20.tar.gz (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from spectral==0.20) (1.21.6)\n",
            "Building wheels for collected packages: spectral\n",
            "  Building wheel for spectral (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spectral: filename=spectral-0.20-py3-none-any.whl size=183936 sha256=5ddcc967ec594ea1e6c65d389c77b206ca7dd9fab20421ed1a132d15f557e961\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/48/cd/916a9ac128df99f05cf8f631433f3b611044634c5ac0db3228\n",
            "Successfully built spectral\n",
            "Installing collected packages: spectral\n",
            "Successfully installed spectral-0.20\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-summary==1.2.0\n",
            "  Downloading torch_summary-1.2.0-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: torch-summary\n",
            "Successfully installed torch-summary-1.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-optimizer==0.0.1a12\n",
            "  Downloading torch_optimizer-0.0.1a12-py3-none-any.whl (33 kB)\n",
            "Collecting pytorch-ranger>=0.1.1\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from torch-optimizer==0.0.1a12) (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.1.0->torch-optimizer==0.0.1a12) (4.4.0)\n",
            "Installing collected packages: pytorch-ranger, torch-optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch-optimizer-0.0.1a12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import collections\n",
        "import math\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn import metrics, preprocessing\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import geniter\n",
        "import record\n",
        "import torch_optimizer as optim2\n",
        "import utils\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "Q987BQ4rcEde"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PARAM_DATASET = 'SV'  # UP,IN,SV, KSC\n",
        "PARAM_EPOCH = 200\n",
        "PARAM_ITER = 3\n",
        "PATCH_SIZE = 4\n",
        "PARAM_VAL = 0.9\n",
        "PARAM_OPTIM = 'adam'\n",
        "PARAM_KERNEL_SIZE = 24"
      ],
      "metadata": {
        "id": "153ERBxwcRtA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# for Monte Carlo runs\n",
        "seeds = [1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341]\n",
        "ensemble = 1\n",
        "\n",
        "global Dataset  # UP,IN,SV, KSC\n",
        "dataset = PARAM_DATASET  # input('Please input the name of Dataset(IN, UP, SV, KSC):')\n",
        "Dataset = dataset.upper()\n",
        "\n",
        "\n",
        "def load_dataset(Dataset, split=0.9):\n",
        "    data_path = '/content/dataset/'\n",
        "    if Dataset == 'IN':\n",
        "        mat_data = sio.loadmat(data_path + 'Indian_pines_corrected.mat')\n",
        "        mat_gt = sio.loadmat(data_path + 'Indian_pines_gt.mat')\n",
        "        data_hsi = mat_data['indian_pines_corrected']\n",
        "        gt_hsi = mat_gt['indian_pines_gt']\n",
        "        K = 200\n",
        "        TOTAL_SIZE = 10249\n",
        "        VALIDATION_SPLIT = split\n",
        "        TRAIN_SIZE = math.ceil(TOTAL_SIZE * VALIDATION_SPLIT)\n",
        "\n",
        "    if Dataset == 'UP':\n",
        "        uPavia = sio.loadmat(data_path + 'PaviaU.mat')\n",
        "        gt_uPavia = sio.loadmat(data_path + 'PaviaU_gt.mat')\n",
        "        data_hsi = uPavia['paviaU']\n",
        "        gt_hsi = gt_uPavia['paviaU_gt']\n",
        "        K = 103\n",
        "        TOTAL_SIZE = 42776\n",
        "        VALIDATION_SPLIT = split\n",
        "        TRAIN_SIZE = math.ceil(TOTAL_SIZE * VALIDATION_SPLIT)\n",
        "\n",
        "    if Dataset == 'SV':\n",
        "        SV = sio.loadmat(data_path + 'Salinas_corrected.mat')\n",
        "        gt_SV = sio.loadmat(data_path + 'Salinas_gt.mat')\n",
        "        data_hsi = SV['salinas_corrected']\n",
        "        gt_hsi = gt_SV['salinas_gt']\n",
        "        K = 15\n",
        "        TOTAL_SIZE = 54129\n",
        "        VALIDATION_SPLIT = split\n",
        "        TRAIN_SIZE = math.ceil(TOTAL_SIZE * VALIDATION_SPLIT)\n",
        "\n",
        "    if Dataset == 'KSC':\n",
        "        SV = sio.loadmat(data_path + 'KSC.mat')\n",
        "        gt_SV = sio.loadmat(data_path + 'KSC_gt.mat')\n",
        "        data_hsi = SV['KSC']\n",
        "        gt_hsi = gt_SV['KSC_gt']\n",
        "        K = data_hsi.shape[2]\n",
        "        TOTAL_SIZE = 5211\n",
        "        VALIDATION_SPLIT = split\n",
        "        TRAIN_SIZE = math.ceil(TOTAL_SIZE * VALIDATION_SPLIT)\n",
        "\n",
        "    shapeor = data_hsi.shape\n",
        "    data_hsi = data_hsi.reshape(-1, data_hsi.shape[-1])\n",
        "    data_hsi = PCA(n_components=K).fit_transform(data_hsi)\n",
        "    shapeor = np.array(shapeor)\n",
        "    shapeor[-1] = K\n",
        "    data_hsi = data_hsi.reshape(shapeor)\n",
        "\n",
        "    return data_hsi, gt_hsi, TOTAL_SIZE, TRAIN_SIZE, VALIDATION_SPLIT\n",
        "\n",
        "\n",
        "# # Pytorch Data Loader Creation\n",
        "\n",
        "data_hsi, gt_hsi, TOTAL_SIZE, TRAIN_SIZE, VALIDATION_SPLIT = load_dataset(\n",
        "    Dataset, PARAM_VAL)\n",
        "print(data_hsi.shape)\n",
        "image_x, image_y, BAND = data_hsi.shape\n",
        "data = data_hsi.reshape(\n",
        "    np.prod(data_hsi.shape[:2]), np.prod(data_hsi.shape[2:]))\n",
        "gt = gt_hsi.reshape(np.prod(gt_hsi.shape[:2]), )\n",
        "CLASSES_NUM = max(gt)\n",
        "print('The class numbers of the HSI data is:', CLASSES_NUM)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJOy5riIcxSO",
        "outputId": "a7652d73-4d1d-4400-bc19-89ae379bbf6c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(512, 217, 15)\n",
            "The class numbers of the HSI data is: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----Importing Setting Parameters-----')\n",
        "ITER = PARAM_ITER\n",
        "PATCH_LENGTH = PATCH_SIZE\n",
        "lr, num_epochs, batch_size = 0.001, 200, 32\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "img_rows = 2 * PATCH_LENGTH + 1\n",
        "img_cols = 2 * PATCH_LENGTH + 1\n",
        "img_channels = data_hsi.shape[2]\n",
        "INPUT_DIMENSION = data_hsi.shape[2]\n",
        "ALL_SIZE = data_hsi.shape[0] * data_hsi.shape[1]\n",
        "VAL_SIZE = int(TRAIN_SIZE)\n",
        "TEST_SIZE = TOTAL_SIZE - TRAIN_SIZE\n",
        "\n",
        "KAPPA = []\n",
        "OA = []\n",
        "AA = []\n",
        "TRAINING_TIME = []\n",
        "TESTING_TIME = []\n",
        "ELEMENT_ACC = np.zeros((ITER, CLASSES_NUM))\n",
        "\n",
        "data = preprocessing.scale(data)\n",
        "data_ = data.reshape(data_hsi.shape[0], data_hsi.shape[1], data_hsi.shape[2])\n",
        "whole_data = data_\n",
        "padded_data = np.lib.pad(\n",
        "    whole_data, ((PATCH_LENGTH, PATCH_LENGTH), (PATCH_LENGTH, PATCH_LENGTH),\n",
        "                 (0, 0)),\n",
        "    'constant',\n",
        "    constant_values=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkrsMqCrc9M9",
        "outputId": "91a6b581-e6af-443c-dd82-6b81808a64f4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Importing Setting Parameters-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Model\n",
        "\n",
        "class LeeEtAl(nn.Module):\n",
        "    \"\"\"\n",
        "    CONTEXTUAL DEEP CNN BASED HYPERSPECTRAL CLASSIFICATION\n",
        "    Hyungtae Lee and Heesung Kwon\n",
        "    IGARSS 2016\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def weight_init(m):\n",
        "        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv3d):\n",
        "            nn.init.kaiming_uniform_(m.weight)\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "    def __init__(self, in_channels, n_classes):\n",
        "        super(LeeEtAl, self).__init__()\n",
        "        # The first convolutional layer applied to the input hyperspectral\n",
        "        # image uses an inception module that locally convolves the input\n",
        "        # image with two convolutional filters with different sizes\n",
        "        # (1x1xB and 3x3xB where B is the number of spectral bands)\n",
        "        self.conv_3x3 = nn.Conv3d(\n",
        "            1, 128, (3, 3, in_channels), stride=(1, 1, 2), padding=(1, 1, 0))\n",
        "        self.conv_1x1 = nn.Conv3d(\n",
        "            1, 128, (1, 1, in_channels), stride=(1, 1, 1), padding=0)\n",
        "        self.name = 'LeeEtAl'\n",
        "\n",
        "        # We use two modules from the residual learning approach\n",
        "        # Residual block 1\n",
        "        self.conv1 = nn.Conv2d(256, 128, (1, 1))\n",
        "        self.conv2 = nn.Conv2d(128, 128, (1, 1))\n",
        "        self.conv3 = nn.Conv2d(128, 128, (1, 1))\n",
        "\n",
        "        # Residual block 2\n",
        "        self.conv4 = nn.Conv2d(128, 128, (1, 1))\n",
        "        self.conv5 = nn.Conv2d(128, 128, (1, 1))\n",
        "\n",
        "        # The layer combination in the last three convolutional layers\n",
        "        # is the same as the fully connected layers of Alexnet\n",
        "        self.conv6 = nn.Conv2d(128, 128, (1, 1))\n",
        "        self.conv7 = nn.Conv2d(128, 128, (1, 1))\n",
        "        self.conv8 = nn.Conv2d(128, n_classes, (9, 9))\n",
        "\n",
        "        self.lrn1 = nn.LocalResponseNorm(256)\n",
        "        self.lrn2 = nn.LocalResponseNorm(128)\n",
        "\n",
        "        # The 7 th and 8 th convolutional layers have dropout in training\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.apply(self.weight_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Inception module\n",
        "        x_3x3 = self.conv_3x3(x)\n",
        "        x_1x1 = self.conv_1x1(x)\n",
        "        x = torch.cat([x_3x3, x_1x1], dim=1)\n",
        "        # Remove the third dimension of the tensor\n",
        "        x = torch.squeeze(x)\n",
        "\n",
        "        # Local Response Normalization\n",
        "        x = F.relu(self.lrn1(x))\n",
        "\n",
        "        # First convolution\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        # Local Response Normalization\n",
        "        x = F.relu(self.lrn2(x))\n",
        "\n",
        "        # First residual block\n",
        "        x_res = F.relu(self.conv2(x))\n",
        "        x_res = self.conv3(x_res)\n",
        "        x = F.relu(x + x_res)\n",
        "\n",
        "        # Second residual block\n",
        "        x_res = F.relu(self.conv4(x))\n",
        "        x_res = self.conv5(x_res)\n",
        "        x = F.relu(x + x_res)\n",
        "\n",
        "        x = F.relu(self.conv6(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.conv7(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv8(x)\n",
        "        x = x.squeeze(2).squeeze(2)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "_oguie3MdQHW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LeeEtAl(BAND, CLASSES_NUM).cuda()\n",
        "\n",
        "summary(model, input_data=(1, img_rows, img_cols, BAND), verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A74ybmghd3lU",
        "outputId": "71438f61-9d0c-4db6-8fbe-662d775104e1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------------------------\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "├─Conv3d: 1-1                            [-1, 128, 9, 9, 1]        17,408\n",
            "├─Conv3d: 1-2                            [-1, 128, 9, 9, 1]        2,048\n",
            "├─LocalResponseNorm: 1-3                 [-1, 256, 9, 9]           --\n",
            "├─Conv2d: 1-4                            [-1, 128, 9, 9]           32,896\n",
            "├─LocalResponseNorm: 1-5                 [-1, 128, 9, 9]           --\n",
            "├─Conv2d: 1-6                            [-1, 128, 9, 9]           16,512\n",
            "├─Conv2d: 1-7                            [-1, 128, 9, 9]           16,512\n",
            "├─Conv2d: 1-8                            [-1, 128, 9, 9]           16,512\n",
            "├─Conv2d: 1-9                            [-1, 128, 9, 9]           16,512\n",
            "├─Conv2d: 1-10                           [-1, 128, 9, 9]           16,512\n",
            "├─Dropout: 1-11                          [-1, 128, 9, 9]           --\n",
            "├─Conv2d: 1-12                           [-1, 128, 9, 9]           16,512\n",
            "├─Dropout: 1-13                          [-1, 128, 9, 9]           --\n",
            "├─Conv2d: 1-14                           [-1, 16, 1, 1]            165,904\n",
            "==========================================================================================\n",
            "Total params: 317,328\n",
            "Trainable params: 317,328\n",
            "Non-trainable params: 0\n",
            "------------------------------------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.71\n",
            "Params size (MB): 1.21\n",
            "Estimated Total Size (MB): 1.93\n",
            "------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "------------------------------------------------------------------------------------------\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "├─Conv3d: 1-1                            [-1, 128, 9, 9, 1]        17,408\n",
              "├─Conv3d: 1-2                            [-1, 128, 9, 9, 1]        2,048\n",
              "├─LocalResponseNorm: 1-3                 [-1, 256, 9, 9]           --\n",
              "├─Conv2d: 1-4                            [-1, 128, 9, 9]           32,896\n",
              "├─LocalResponseNorm: 1-5                 [-1, 128, 9, 9]           --\n",
              "├─Conv2d: 1-6                            [-1, 128, 9, 9]           16,512\n",
              "├─Conv2d: 1-7                            [-1, 128, 9, 9]           16,512\n",
              "├─Conv2d: 1-8                            [-1, 128, 9, 9]           16,512\n",
              "├─Conv2d: 1-9                            [-1, 128, 9, 9]           16,512\n",
              "├─Conv2d: 1-10                           [-1, 128, 9, 9]           16,512\n",
              "├─Dropout: 1-11                          [-1, 128, 9, 9]           --\n",
              "├─Conv2d: 1-12                           [-1, 128, 9, 9]           16,512\n",
              "├─Dropout: 1-13                          [-1, 128, 9, 9]           --\n",
              "├─Conv2d: 1-14                           [-1, 16, 1, 1]            165,904\n",
              "==========================================================================================\n",
              "Total params: 317,328\n",
              "Trainable params: 317,328\n",
              "Non-trainable params: 0\n",
              "------------------------------------------------------------------------------------------\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.71\n",
              "Params size (MB): 1.21\n",
              "Estimated Total Size (MB): 1.93\n",
              "------------------------------------------------------------------------------------------"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plotting\n",
        "\n",
        "\n",
        "def train(net,\n",
        "          train_iter,\n",
        "          valida_iter,\n",
        "          loss,\n",
        "          optimizer,\n",
        "          device,\n",
        "          epochs,\n",
        "          early_stopping=True,\n",
        "          early_num=20):\n",
        "    loss_list = [100]\n",
        "    early_epoch = 0\n",
        "\n",
        "    net = net.to(device)\n",
        "    print(\"training on \", device)\n",
        "    start = time.time()\n",
        "    train_loss_list = []\n",
        "    valida_loss_list = []\n",
        "    train_acc_list = []\n",
        "    valida_acc_list = []\n",
        "    for epoch in range(epochs):\n",
        "        train_acc_sum, n = 0.0, 0\n",
        "        time_epoch = time.time()\n",
        "        lr_adjust = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer, 15, eta_min=0.0, last_epoch=-1)\n",
        "        for X, y in train_iter:\n",
        "\n",
        "            batch_count, train_l_sum = 0, 0\n",
        "            #X = X.permute(0, 3, 1, 2)\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            y_hat = net(X)\n",
        "            # print('y_hat', y_hat)\n",
        "            # print('y', y)\n",
        "            l = loss(y_hat, y.long())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            l.backward()\n",
        "            optimizer.step()\n",
        "            train_l_sum += l.cpu().item()\n",
        "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
        "            n += y.shape[0]\n",
        "            batch_count += 1\n",
        "        lr_adjust.step()\n",
        "        valida_acc, valida_loss = record.evaluate_accuracy(\n",
        "            valida_iter, net, loss, device)\n",
        "        loss_list.append(valida_loss)\n",
        "\n",
        "        train_loss_list.append(train_l_sum)  # / batch_count)\n",
        "        train_acc_list.append(train_acc_sum / n)\n",
        "        valida_loss_list.append(valida_loss)\n",
        "        valida_acc_list.append(valida_acc)\n",
        "\n",
        "        print(\n",
        "            'epoch %d, train loss %.6f, train acc %.3f, valida loss %.6f, valida acc %.3f, time %.1f sec'\n",
        "            % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n,\n",
        "               valida_loss, valida_acc, time.time() - time_epoch))\n",
        "\n",
        "        PATH = \"./net_DBA.pt\"\n",
        "        # if loss_list[-1] <= 0.01 and valida_acc >= 0.95:\n",
        "        #     torch.save(net.state_dict(), PATH)\n",
        "        #     break\n",
        "\n",
        "        if early_stopping and loss_list[-2] < loss_list[-1]:\n",
        "            if early_epoch == 0:  # and valida_acc > 0.9:\n",
        "                torch.save(net.state_dict(), PATH)\n",
        "            early_epoch += 1\n",
        "            loss_list[-1] = loss_list[-2]\n",
        "            if early_epoch == early_num:\n",
        "                net.load_state_dict(torch.load(PATH))\n",
        "                break\n",
        "        else:\n",
        "            early_epoch = 0\n",
        "\n",
        "    print('epoch %d, loss %.4f, train acc %.3f, time %.1f sec'\n",
        "          % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n,\n",
        "             time.time() - start))\n",
        "\n",
        "\n",
        "def sampling(proportion, ground_truth):\n",
        "    train = {}\n",
        "    test = {}\n",
        "    labels_loc = {}\n",
        "    m = max(ground_truth)\n",
        "    for i in range(m):\n",
        "        indexes = [\n",
        "            j for j, x in enumerate(ground_truth.ravel().tolist())\n",
        "            if x == i + 1\n",
        "        ]\n",
        "        np.random.shuffle(indexes)\n",
        "        labels_loc[i] = indexes\n",
        "        if proportion != 1:\n",
        "            nb_val = max(int((1 - proportion) * len(indexes)), 3)\n",
        "        else:\n",
        "            nb_val = 0\n",
        "        train[i] = indexes[:nb_val]\n",
        "        test[i] = indexes[nb_val:]\n",
        "    train_indexes = []\n",
        "    test_indexes = []\n",
        "    for i in range(m):\n",
        "        train_indexes += train[i]\n",
        "        test_indexes += test[i]\n",
        "    np.random.shuffle(train_indexes)\n",
        "    np.random.shuffle(test_indexes)\n",
        "    return train_indexes, test_indexes\n",
        "\n",
        "\n",
        "def select(groundTruth):  #divide dataset into train and test datasets\n",
        "    labels_loc = {}\n",
        "    train = {}\n",
        "    test = {}\n",
        "    m = max(groundTruth)\n",
        "    #amount = [3, 41, 29, 7, 14, 20, 2, 15, 3, 36, 64, 22, 4, 28, 10, 2]\n",
        "    #amount = [43, 1387, 801, 230, 469, 710, 26, 463, 17, 936, 2391, 571, 201, 1237, 376, 91]\n",
        "    if Dataset == 'IN':\n",
        "        amount = [\n",
        "            35, 1011, 581, 167, 344, 515, 19, 327, 12, 683, 1700, 418, 138,\n",
        "            876, 274, 69\n",
        "        ]  #IP 20%\n",
        "    #amount = [6, 144, 84, 24, 50, 75, 3, 49, 2, 97, 247, 62, 22, 130, 38, 10]   #IP 20%\n",
        "    if Dataset == 'SV':\n",
        "        amount = [\n",
        "            1573, 2920, 1549, 1093, 2099, 3103, 3805, 8836, 4861, 2570, 837, 1509, 717,\n",
        "            838, 5697, 1415\n",
        "        ]\n",
        "    if Dataset == 'UP':\n",
        "        amount = [5297, 14974, 1648, 2424, 1076, 4026, 1046, 2950, 755]  #UP\n",
        "    if Dataset == 'KSC':\n",
        "        amount = [\n",
        "            530, 165, 176, 170, 110, 161, 80, 299, 377, 283, 296, 341, 654\n",
        "        ]  #KSC\n",
        "    for i in range(m):\n",
        "        indices = [\n",
        "            j for j, x in enumerate(groundTruth.ravel().tolist()) if x == i + 1\n",
        "        ]\n",
        "        np.random.shuffle(indices)\n",
        "        labels_loc[i] = indices\n",
        "        nb_val = int(amount[i])\n",
        "        train[i] = indices[:-nb_val]\n",
        "        test[i] = indices[-nb_val:]\n",
        "#    whole_indices = []\n",
        "    train_indices = []\n",
        "    test_indices = []\n",
        "    for i in range(m):\n",
        "        #        whole_indices += labels_loc[i]\n",
        "        train_indices += train[i]\n",
        "        test_indices += test[i]\n",
        "    np.random.shuffle(train_indices)\n",
        "    np.random.shuffle(test_indices)\n",
        "    return train_indices, test_indices"
      ],
      "metadata": {
        "id": "yeobIE8BhVWA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Training\n",
        "\n",
        "for index_iter in range(ITER):\n",
        "    print('iter:', index_iter)\n",
        "    #define the model\n",
        "    #net = pResNet(32, 48, CLASSES_NUM, BAND, 2, 16, bottleneck=True)\n",
        "    #net = resnet20(num_classes=CLASSES_NUM)\n",
        "    net = LeeEtAl(BAND, CLASSES_NUM)\n",
        "\n",
        "    if PARAM_OPTIM == 'diffgrad':\n",
        "        optimizer = optim2.DiffGrad(\n",
        "            net.parameters(),\n",
        "            lr=lr,\n",
        "            betas=(0.9, 0.999),\n",
        "            eps=1e-8,\n",
        "            weight_decay=0)  # weight_decay=0.0001)\n",
        "    if PARAM_OPTIM == 'adam':\n",
        "        optimizer = optim.Adam(\n",
        "            net.parameters(),\n",
        "            lr=1e-3,\n",
        "            betas=(0.9, 0.999),\n",
        "            eps=1e-8,\n",
        "            weight_decay=0)\n",
        "    time_1 = int(time.time())\n",
        "    np.random.seed(seeds[index_iter])\n",
        "    # train_indices, test_indices = select(gt)\n",
        "    train_indices, test_indices = sampling(VALIDATION_SPLIT, gt)\n",
        "    _, total_indices = sampling(1, gt)\n",
        "\n",
        "    TRAIN_SIZE = len(train_indices)\n",
        "    print('Train size: ', TRAIN_SIZE)\n",
        "    TEST_SIZE = TOTAL_SIZE - TRAIN_SIZE\n",
        "    print('Test size: ', TEST_SIZE)\n",
        "    VAL_SIZE = int(TRAIN_SIZE)\n",
        "    print('Validation size: ', VAL_SIZE)\n",
        "\n",
        "    print('-----Selecting Small Pieces from the Original Cube Data-----')\n",
        "    train_iter, valida_iter, test_iter, all_iter = geniter.generate_iter(\n",
        "        TRAIN_SIZE, train_indices, TEST_SIZE, test_indices, TOTAL_SIZE,\n",
        "        total_indices, VAL_SIZE, whole_data, PATCH_LENGTH, padded_data,\n",
        "        INPUT_DIMENSION, 16, gt)  #batchsize in 1\n",
        "\n",
        "    tic1 = time.time()\n",
        "    train(\n",
        "        net,\n",
        "        train_iter,\n",
        "        valida_iter,\n",
        "        loss,\n",
        "        optimizer,\n",
        "        device,\n",
        "        epochs=PARAM_EPOCH)\n",
        "    toc1 = time.time()\n",
        "\n",
        "    pred_test = []\n",
        "    tic2 = time.time()\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_iter:\n",
        "            #print('Shape of X',X.shape)\n",
        "            #X = X.permute(0, 3, 1, 2)\n",
        "            X = X.to(device)\n",
        "            net.eval()\n",
        "            y_hat = net(X)\n",
        "            pred_test.extend(np.array(net(X).cpu().argmax(axis=1)))\n",
        "    toc2 = time.time()\n",
        "    collections.Counter(pred_test)\n",
        "    gt_test = gt[test_indices] - 1\n",
        "\n",
        "    overall_acc = metrics.accuracy_score(pred_test, gt_test[:-VAL_SIZE])\n",
        "    confusion_matrix = metrics.confusion_matrix(pred_test, gt_test[:-VAL_SIZE])\n",
        "    each_acc, average_acc = record.aa_and_each_accuracy(confusion_matrix)\n",
        "    kappa = metrics.cohen_kappa_score(pred_test, gt_test[:-VAL_SIZE])\n",
        "\n",
        "    torch.save(\n",
        "        net.state_dict(),\n",
        "        \"./model/\" + 'ContextualNet' + str(round(overall_acc, 3)) + '.pt')\n",
        "    KAPPA.append(kappa)\n",
        "    OA.append(overall_acc)\n",
        "    AA.append(average_acc)\n",
        "    TRAINING_TIME.append(toc1 - tic1)\n",
        "    TESTING_TIME.append(toc2 - tic2)\n",
        "    ELEMENT_ACC[index_iter, :] = each_acc\n",
        "\n",
        "# # Map, Records\n",
        "print(\"--------\" + \" Training Finished-----------\")\n",
        "record.record_output(\n",
        "    OA, AA, KAPPA, ELEMENT_ACC, TRAINING_TIME, TESTING_TIME,\n",
        "    './report/' + 'ContextualNetpatch:' + str(img_rows) + '_' + Dataset +\n",
        "    'split' + str(VALIDATION_SPLIT) + 'lr' + str(lr) + PARAM_OPTIM + '.txt')\n",
        "\n",
        "utils.generate_png(\n",
        "    all_iter, net, gt_hsi, Dataset, device, total_indices,\n",
        "    './classification_maps/' + 'ContextualNetpatch:' + str(img_rows) + '_' +\n",
        "    Dataset + 'split' + str(VALIDATION_SPLIT) + 'lr' + str(lr) + PARAM_OPTIM)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tuhgr7K-hnXg",
        "outputId": "d107b0aa-693b-4e14-c72f-d5a0e4cc9340"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 0\n",
            "Train size:  5402\n",
            "Test size:  48727\n",
            "Validation size:  5402\n",
            "-----Selecting Small Pieces from the Original Cube Data-----\n",
            "(5402, 9, 9, 15)\n",
            "training on  cuda\n",
            "epoch 1, train loss 0.001451, train acc 0.860, valida loss 0.049251, valida acc 0.958, time 6.0 sec\n",
            "epoch 2, train loss 0.012013, train acc 0.973, valida loss 0.021709, valida acc 0.986, time 4.9 sec\n",
            "epoch 3, train loss 0.004666, train acc 0.982, valida loss 0.015262, valida acc 0.990, time 4.9 sec\n",
            "epoch 4, train loss 0.000113, train acc 0.984, valida loss 0.030115, valida acc 0.991, time 4.9 sec\n",
            "epoch 5, train loss 0.000289, train acc 0.991, valida loss 0.001052, valida acc 0.996, time 5.1 sec\n",
            "epoch 6, train loss 0.000001, train acc 0.996, valida loss 0.000004, valida acc 0.996, time 4.9 sec\n",
            "epoch 7, train loss 0.000345, train acc 0.993, valida loss 0.001139, valida acc 0.996, time 4.9 sec\n",
            "epoch 8, train loss 0.000192, train acc 0.990, valida loss 0.013700, valida acc 0.989, time 5.0 sec\n",
            "epoch 9, train loss 0.000014, train acc 0.999, valida loss 0.002356, valida acc 0.987, time 4.9 sec\n",
            "epoch 10, train loss 0.060941, train acc 0.995, valida loss 0.002442, valida acc 0.976, time 4.9 sec\n",
            "epoch 11, train loss 0.000014, train acc 0.993, valida loss 0.008827, valida acc 0.993, time 4.9 sec\n",
            "epoch 12, train loss 0.000287, train acc 0.999, valida loss 0.000013, valida acc 0.998, time 5.0 sec\n",
            "epoch 13, train loss 0.000000, train acc 0.999, valida loss 0.000002, valida acc 0.994, time 5.0 sec\n",
            "epoch 14, train loss 0.000000, train acc 0.998, valida loss 0.002473, valida acc 0.994, time 5.3 sec\n",
            "epoch 15, train loss 0.000001, train acc 0.993, valida loss 0.000002, valida acc 0.998, time 5.2 sec\n",
            "epoch 16, train loss 0.000012, train acc 1.000, valida loss 0.053032, valida acc 0.999, time 5.0 sec\n",
            "epoch 17, train loss 0.002737, train acc 1.000, valida loss 0.000148, valida acc 0.999, time 5.0 sec\n",
            "epoch 18, train loss 0.000291, train acc 1.000, valida loss 0.000003, valida acc 0.998, time 5.0 sec\n",
            "epoch 19, train loss 0.000002, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.0 sec\n",
            "epoch 20, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.0 sec\n",
            "epoch 21, train loss 0.000000, train acc 1.000, valida loss 0.000003, valida acc 0.999, time 5.0 sec\n",
            "epoch 22, train loss 0.000000, train acc 1.000, valida loss 0.032691, valida acc 0.998, time 5.0 sec\n",
            "epoch 23, train loss 0.000000, train acc 1.000, valida loss 0.000008, valida acc 0.998, time 5.1 sec\n",
            "epoch 24, train loss 0.000000, train acc 1.000, valida loss 0.000008, valida acc 0.999, time 5.1 sec\n",
            "epoch 25, train loss 0.000000, train acc 1.000, valida loss 0.000005, valida acc 0.996, time 5.1 sec\n",
            "epoch 26, train loss 0.052766, train acc 0.986, valida loss 0.000055, valida acc 0.984, time 5.1 sec\n",
            "epoch 27, train loss 0.000011, train acc 0.995, valida loss 0.000000, valida acc 0.998, time 5.1 sec\n",
            "epoch 28, train loss 0.000925, train acc 1.000, valida loss 0.000003, valida acc 0.998, time 5.1 sec\n",
            "epoch 29, train loss 0.000001, train acc 1.000, valida loss 0.000010, valida acc 0.998, time 5.1 sec\n",
            "epoch 30, train loss 0.000003, train acc 0.999, valida loss 0.000000, valida acc 0.996, time 5.1 sec\n",
            "epoch 31, train loss 0.000001, train acc 0.999, valida loss 0.009196, valida acc 0.998, time 5.1 sec\n",
            "epoch 32, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.1 sec\n",
            "epoch 33, train loss 0.000065, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 34, train loss 0.000018, train acc 1.000, valida loss 0.098327, valida acc 0.998, time 5.2 sec\n",
            "epoch 35, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.1 sec\n",
            "epoch 36, train loss 0.000002, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.1 sec\n",
            "epoch 37, train loss 0.000104, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.1 sec\n",
            "epoch 38, train loss 0.000003, train acc 1.000, valida loss 0.000070, valida acc 0.999, time 5.1 sec\n",
            "epoch 39, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.1 sec\n",
            "epoch 40, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.1 sec\n",
            "epoch 41, train loss 0.000019, train acc 0.998, valida loss 0.271013, valida acc 0.984, time 5.2 sec\n",
            "epoch 42, train loss 0.000000, train acc 0.995, valida loss 0.000001, valida acc 0.997, time 5.2 sec\n",
            "epoch 43, train loss 0.000000, train acc 0.999, valida loss 0.000000, valida acc 0.997, time 5.2 sec\n",
            "epoch 44, train loss 0.000000, train acc 0.998, valida loss 0.000001, valida acc 0.994, time 5.2 sec\n",
            "epoch 45, train loss 0.000000, train acc 0.999, valida loss 0.000000, valida acc 0.997, time 5.2 sec\n",
            "epoch 46, train loss 0.000024, train acc 1.000, valida loss 0.000000, valida acc 0.990, time 5.2 sec\n",
            "epoch 47, train loss 0.000104, train acc 0.999, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 48, train loss 0.000000, train acc 0.998, valida loss 0.006752, valida acc 0.994, time 5.2 sec\n",
            "epoch 49, train loss 0.000000, train acc 0.999, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 50, train loss 0.000000, train acc 0.999, valida loss 0.000011, valida acc 0.999, time 5.2 sec\n",
            "epoch 51, train loss 0.000000, train acc 1.000, valida loss 0.000001, valida acc 0.998, time 5.2 sec\n",
            "epoch 52, train loss 0.000001, train acc 1.000, valida loss 0.000001, valida acc 0.998, time 5.2 sec\n",
            "epoch 53, train loss 0.000015, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 54, train loss 0.000001, train acc 1.000, valida loss 0.000001, valida acc 0.998, time 5.2 sec\n",
            "epoch 55, train loss 0.000000, train acc 0.999, valida loss 0.000000, valida acc 0.997, time 5.2 sec\n",
            "epoch 56, train loss 0.000000, train acc 0.999, valida loss 0.000000, valida acc 0.997, time 5.2 sec\n",
            "epoch 57, train loss 0.000970, train acc 0.997, valida loss 0.000049, valida acc 0.997, time 5.2 sec\n",
            "epoch 58, train loss 0.000025, train acc 0.999, valida loss 0.003169, valida acc 0.998, time 5.2 sec\n",
            "epoch 59, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 60, train loss 0.000036, train acc 0.999, valida loss 0.000204, valida acc 0.994, time 5.2 sec\n",
            "epoch 61, train loss 0.000000, train acc 0.999, valida loss 0.000001, valida acc 0.998, time 5.2 sec\n",
            "epoch 62, train loss 0.000000, train acc 0.999, valida loss 0.000000, valida acc 0.997, time 5.3 sec\n",
            "epoch 63, train loss 0.000000, train acc 1.000, valida loss 0.000001, valida acc 0.999, time 5.2 sec\n",
            "epoch 64, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 65, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 66, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 67, train loss 0.000003, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 68, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 69, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 70, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 71, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 72, train loss 0.000266, train acc 1.000, valida loss 0.000000, valida acc 0.996, time 5.2 sec\n",
            "epoch 73, train loss 0.000007, train acc 0.997, valida loss 0.000010, valida acc 0.998, time 5.2 sec\n",
            "epoch 74, train loss 0.000000, train acc 1.000, valida loss 0.000053, valida acc 0.998, time 5.2 sec\n",
            "epoch 75, train loss 0.000003, train acc 1.000, valida loss 0.000295, valida acc 0.997, time 5.2 sec\n",
            "epoch 76, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.997, time 5.2 sec\n",
            "epoch 77, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 78, train loss 0.000000, train acc 1.000, valida loss 0.000001, valida acc 0.998, time 5.2 sec\n",
            "epoch 79, train loss 0.000000, train acc 1.000, valida loss 0.000066, valida acc 0.998, time 5.2 sec\n",
            "epoch 80, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 81, train loss 0.000000, train acc 0.999, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 82, train loss 0.000000, train acc 1.000, valida loss 0.000407, valida acc 0.998, time 5.2 sec\n",
            "epoch 83, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 84, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 85, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 86, train loss 0.000013, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 87, train loss 0.060650, train acc 0.998, valida loss 0.000000, valida acc 0.994, time 5.2 sec\n",
            "epoch 88, train loss 0.000010, train acc 1.000, valida loss 0.000001, valida acc 0.998, time 5.2 sec\n",
            "epoch 89, train loss 0.000000, train acc 1.000, valida loss 0.000072, valida acc 0.998, time 5.2 sec\n",
            "epoch 90, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.3 sec\n",
            "epoch 91, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 92, train loss 0.000000, train acc 1.000, valida loss 0.000001, valida acc 0.998, time 5.2 sec\n",
            "epoch 93, train loss 0.000003, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 94, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 95, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 96, train loss 0.000000, train acc 1.000, valida loss 0.000002, valida acc 0.999, time 5.2 sec\n",
            "epoch 97, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 98, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 99, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 100, train loss 0.000000, train acc 1.000, valida loss 0.000021, valida acc 0.999, time 5.2 sec\n",
            "epoch 101, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 102, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 103, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 104, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 105, train loss 0.000002, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 106, train loss 0.000001, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 107, train loss 0.000000, train acc 1.000, valida loss 0.000011, valida acc 0.999, time 5.2 sec\n",
            "epoch 108, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 109, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 110, train loss 0.000000, train acc 1.000, valida loss 0.000288, valida acc 0.999, time 5.2 sec\n",
            "epoch 111, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.3 sec\n",
            "epoch 112, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 113, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 114, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 115, train loss 0.000000, train acc 1.000, valida loss 0.000370, valida acc 0.999, time 5.2 sec\n",
            "epoch 116, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 117, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.3 sec\n",
            "epoch 118, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.3 sec\n",
            "epoch 119, train loss 0.000000, train acc 1.000, valida loss 0.004743, valida acc 0.999, time 5.2 sec\n",
            "epoch 120, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 121, train loss 0.000000, train acc 1.000, valida loss 0.000088, valida acc 0.999, time 5.2 sec\n",
            "epoch 122, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 123, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 124, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 125, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 126, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 127, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 128, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 129, train loss 0.000000, train acc 1.000, valida loss 0.000095, valida acc 0.999, time 5.2 sec\n",
            "epoch 130, train loss 0.000000, train acc 1.000, valida loss 0.000374, valida acc 0.999, time 5.2 sec\n",
            "epoch 131, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 132, train loss 0.000000, train acc 1.000, valida loss 0.000002, valida acc 0.999, time 5.2 sec\n",
            "epoch 133, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 134, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 135, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 136, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 137, train loss 0.000000, train acc 1.000, valida loss 0.079693, valida acc 0.999, time 5.2 sec\n",
            "epoch 138, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 139, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 140, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 141, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 142, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 143, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 144, train loss 0.000000, train acc 1.000, valida loss 0.000430, valida acc 0.999, time 5.2 sec\n",
            "epoch 145, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 146, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.3 sec\n",
            "epoch 147, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 148, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 149, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 150, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 151, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 152, train loss 0.000000, train acc 1.000, valida loss 0.024621, valida acc 0.999, time 5.2 sec\n",
            "epoch 153, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 154, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 155, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 156, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 157, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 158, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 159, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 160, train loss 0.000000, train acc 1.000, valida loss 0.000002, valida acc 0.999, time 5.2 sec\n",
            "epoch 161, train loss 0.000000, train acc 1.000, valida loss 0.000041, valida acc 0.999, time 5.2 sec\n",
            "epoch 162, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 163, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 164, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 165, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 166, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 167, train loss 0.000000, train acc 1.000, valida loss 0.000018, valida acc 0.999, time 5.2 sec\n",
            "epoch 168, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 1.000, time 5.2 sec\n",
            "epoch 169, train loss 0.000000, train acc 0.999, valida loss 0.000000, valida acc 0.996, time 5.2 sec\n",
            "epoch 170, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 171, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 172, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 173, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 174, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.3 sec\n",
            "epoch 175, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 176, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 177, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 178, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 179, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 180, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 181, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 182, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 183, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 184, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 185, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 186, train loss 0.000000, train acc 1.000, valida loss 0.000514, valida acc 0.999, time 5.2 sec\n",
            "epoch 187, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 188, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 189, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 190, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 191, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 192, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 193, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 194, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 195, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 196, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 197, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 198, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 199, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 200, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 200, loss 0.0000, train acc 1.000, time 1037.4 sec\n",
            "iter: 1\n",
            "Train size:  5402\n",
            "Test size:  48727\n",
            "Validation size:  5402\n",
            "-----Selecting Small Pieces from the Original Cube Data-----\n",
            "(5402, 9, 9, 15)\n",
            "training on  cuda\n",
            "epoch 1, train loss 0.036852, train acc 0.855, valida loss 0.034492, valida acc 0.973, time 5.3 sec\n",
            "epoch 2, train loss 0.016619, train acc 0.966, valida loss 0.004049, valida acc 0.987, time 5.2 sec\n",
            "epoch 3, train loss 0.012743, train acc 0.982, valida loss 0.017207, valida acc 0.977, time 5.2 sec\n",
            "epoch 4, train loss 0.006511, train acc 0.989, valida loss 0.000203, valida acc 0.991, time 5.2 sec\n",
            "epoch 5, train loss 0.002523, train acc 0.984, valida loss 0.000999, valida acc 0.983, time 5.2 sec\n",
            "epoch 6, train loss 0.000191, train acc 0.995, valida loss 0.063132, valida acc 0.978, time 5.2 sec\n",
            "epoch 7, train loss 0.000014, train acc 0.995, valida loss 0.000005, valida acc 0.997, time 5.2 sec\n",
            "epoch 8, train loss 0.000001, train acc 0.992, valida loss 0.027182, valida acc 0.998, time 5.2 sec\n",
            "epoch 9, train loss 0.055681, train acc 0.998, valida loss 0.006362, valida acc 0.997, time 5.2 sec\n",
            "epoch 10, train loss 0.001029, train acc 0.999, valida loss 0.000915, valida acc 0.998, time 5.2 sec\n",
            "epoch 11, train loss 0.002091, train acc 0.994, valida loss 0.000845, valida acc 0.993, time 5.2 sec\n",
            "epoch 12, train loss 0.001154, train acc 0.991, valida loss 0.001614, valida acc 0.996, time 5.2 sec\n",
            "epoch 13, train loss 0.000011, train acc 0.999, valida loss 0.092536, valida acc 0.991, time 5.2 sec\n",
            "epoch 14, train loss 0.000431, train acc 0.995, valida loss 0.014682, valida acc 0.998, time 5.2 sec\n",
            "epoch 15, train loss 0.000022, train acc 0.997, valida loss 0.000603, valida acc 0.998, time 5.2 sec\n",
            "epoch 16, train loss 0.000206, train acc 0.994, valida loss 0.000085, valida acc 0.996, time 5.2 sec\n",
            "epoch 17, train loss 0.000053, train acc 0.999, valida loss 0.010059, valida acc 0.999, time 5.2 sec\n",
            "epoch 18, train loss 0.000314, train acc 1.000, valida loss 0.000030, valida acc 0.999, time 5.2 sec\n",
            "epoch 19, train loss 0.000006, train acc 1.000, valida loss 0.000001, valida acc 0.999, time 5.2 sec\n",
            "epoch 20, train loss 0.000016, train acc 1.000, valida loss 0.000000, valida acc 0.997, time 5.2 sec\n",
            "epoch 21, train loss 0.000000, train acc 1.000, valida loss 0.000001, valida acc 0.999, time 5.2 sec\n",
            "epoch 22, train loss 0.003065, train acc 0.996, valida loss 0.003496, valida acc 0.989, time 5.2 sec\n",
            "epoch 23, train loss 0.000006, train acc 0.993, valida loss 0.001416, valida acc 0.999, time 5.3 sec\n",
            "epoch 24, train loss 0.000224, train acc 1.000, valida loss 0.000065, valida acc 0.999, time 5.3 sec\n",
            "epoch 25, train loss 0.000143, train acc 1.000, valida loss 0.000032, valida acc 0.999, time 5.2 sec\n",
            "epoch 26, train loss 0.000000, train acc 1.000, valida loss 0.000133, valida acc 0.999, time 5.2 sec\n",
            "epoch 27, train loss 0.000001, train acc 0.999, valida loss 0.002075, valida acc 0.999, time 5.2 sec\n",
            "epoch 28, train loss 0.000010, train acc 1.000, valida loss 0.000016, valida acc 0.999, time 5.2 sec\n",
            "epoch 29, train loss 0.000017, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 30, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 31, train loss 0.000001, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 32, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 33, train loss 0.000000, train acc 0.997, valida loss 0.000043, valida acc 0.994, time 5.2 sec\n",
            "epoch 34, train loss 0.000000, train acc 0.997, valida loss 0.000007, valida acc 0.997, time 5.2 sec\n",
            "epoch 35, train loss 0.000012, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 36, train loss 0.000208, train acc 0.997, valida loss 0.000001, valida acc 0.998, time 5.2 sec\n",
            "epoch 37, train loss 0.000000, train acc 1.000, valida loss 0.000225, valida acc 0.999, time 5.2 sec\n",
            "epoch 38, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 39, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 40, train loss 0.000000, train acc 1.000, valida loss 0.000001, valida acc 0.999, time 5.2 sec\n",
            "epoch 41, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 42, train loss 0.005850, train acc 0.993, valida loss 0.000010, valida acc 0.987, time 5.2 sec\n",
            "epoch 43, train loss 0.000001, train acc 0.998, valida loss 0.000168, valida acc 0.998, time 5.2 sec\n",
            "epoch 44, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 45, train loss 0.000020, train acc 1.000, valida loss 0.000018, valida acc 0.998, time 5.2 sec\n",
            "epoch 46, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 47, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 48, train loss 0.000003, train acc 1.000, valida loss 0.000829, valida acc 0.998, time 5.2 sec\n",
            "epoch 49, train loss 0.000056, train acc 1.000, valida loss 0.000102, valida acc 1.000, time 5.2 sec\n",
            "epoch 50, train loss 0.000001, train acc 1.000, valida loss 0.000000, valida acc 0.997, time 5.2 sec\n",
            "epoch 51, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 1.000, time 5.2 sec\n",
            "epoch 52, train loss 0.000048, train acc 1.000, valida loss 0.000000, valida acc 1.000, time 5.3 sec\n",
            "epoch 53, train loss 0.000001, train acc 1.000, valida loss 0.000004, valida acc 1.000, time 5.2 sec\n",
            "epoch 54, train loss 0.000000, train acc 1.000, valida loss 0.000028, valida acc 0.999, time 5.2 sec\n",
            "epoch 55, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 1.000, time 5.2 sec\n",
            "epoch 56, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 1.000, time 5.2 sec\n",
            "epoch 57, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.996, time 5.2 sec\n",
            "epoch 58, train loss 0.005293, train acc 0.993, valida loss 0.000089, valida acc 0.987, time 5.2 sec\n",
            "epoch 59, train loss 0.000000, train acc 0.999, valida loss 0.000000, valida acc 0.997, time 5.2 sec\n",
            "epoch 60, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 61, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 62, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 63, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 64, train loss 0.000002, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 65, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 66, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 67, train loss 0.000022, train acc 1.000, valida loss 0.000001, valida acc 0.999, time 5.2 sec\n",
            "epoch 68, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 69, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 70, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 71, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 72, train loss 0.000000, train acc 1.000, valida loss 0.079671, valida acc 0.999, time 5.2 sec\n",
            "epoch 73, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 74, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 75, train loss 0.000007, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 76, train loss 0.000000, train acc 0.998, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 77, train loss 0.000099, train acc 0.998, valida loss 0.000000, valida acc 0.993, time 5.2 sec\n",
            "epoch 78, train loss 0.000000, train acc 0.999, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 79, train loss 0.000000, train acc 1.000, valida loss 0.226096, valida acc 0.997, time 5.2 sec\n",
            "epoch 80, train loss 0.000004, train acc 1.000, valida loss 0.000002, valida acc 0.999, time 5.3 sec\n",
            "epoch 81, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 82, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 83, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 84, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 85, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 86, train loss 0.000000, train acc 1.000, valida loss 0.000057, valida acc 0.999, time 5.2 sec\n",
            "epoch 87, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 88, train loss 0.000002, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 89, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 90, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 91, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 92, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 93, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 94, train loss 0.000000, train acc 1.000, valida loss 0.000314, valida acc 0.999, time 5.2 sec\n",
            "epoch 95, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 96, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 97, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 98, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 99, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 100, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 101, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 102, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 103, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 104, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 105, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 106, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 107, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.3 sec\n",
            "epoch 108, train loss 0.000000, train acc 0.999, valida loss 0.000000, valida acc 0.998, time 5.3 sec\n",
            "epoch 109, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 110, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 111, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 112, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 113, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 114, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 115, train loss 0.000001, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 116, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 117, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 118, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 119, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 120, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 121, train loss 0.000000, train acc 1.000, valida loss 0.000051, valida acc 0.999, time 5.2 sec\n",
            "epoch 122, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 123, train loss 0.000000, train acc 1.000, valida loss 0.000368, valida acc 0.999, time 5.2 sec\n",
            "epoch 124, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 125, train loss 0.000000, train acc 1.000, valida loss 0.000004, valida acc 0.999, time 5.2 sec\n",
            "epoch 126, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 127, train loss 0.000012, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 128, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 129, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 130, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 131, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 132, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 133, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 134, train loss 0.000000, train acc 1.000, valida loss 0.081236, valida acc 0.999, time 5.2 sec\n",
            "epoch 135, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.3 sec\n",
            "epoch 136, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 1.000, time 5.2 sec\n",
            "epoch 137, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 138, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 139, train loss 0.000000, train acc 0.999, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 140, train loss 0.000000, train acc 1.000, valida loss 0.000107, valida acc 0.998, time 5.2 sec\n",
            "epoch 141, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 142, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 143, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 144, train loss 0.000000, train acc 1.000, valida loss 0.019675, valida acc 0.999, time 5.2 sec\n",
            "epoch 145, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 146, train loss 0.000000, train acc 1.000, valida loss 0.000001, valida acc 0.999, time 5.2 sec\n",
            "epoch 147, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 148, train loss 0.000000, train acc 1.000, valida loss 0.106647, valida acc 0.999, time 5.2 sec\n",
            "epoch 149, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 150, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 151, train loss 0.000000, train acc 1.000, valida loss 0.000002, valida acc 0.999, time 5.2 sec\n",
            "epoch 152, train loss 0.000000, train acc 1.000, valida loss 0.000001, valida acc 0.999, time 5.2 sec\n",
            "epoch 153, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 154, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 155, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 156, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 157, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 158, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 159, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 160, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 161, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 162, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 163, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.3 sec\n",
            "epoch 164, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 165, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 166, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 167, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 168, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 169, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 170, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 171, train loss 0.000000, train acc 1.000, valida loss 0.000001, valida acc 0.999, time 5.2 sec\n",
            "epoch 172, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 173, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 174, train loss 0.000000, train acc 1.000, valida loss 0.000004, valida acc 0.999, time 5.2 sec\n",
            "epoch 175, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 176, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 177, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 178, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 179, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 180, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 181, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 182, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 183, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 184, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 185, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 186, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 187, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 188, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 189, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 190, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 191, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.3 sec\n",
            "epoch 192, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 193, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 194, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 195, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 196, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 197, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 198, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 199, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 1.000, time 5.2 sec\n",
            "epoch 200, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 1.000, time 5.2 sec\n",
            "epoch 200, loss 0.0000, train acc 1.000, time 1043.0 sec\n",
            "iter: 2\n",
            "Train size:  5402\n",
            "Test size:  48727\n",
            "Validation size:  5402\n",
            "-----Selecting Small Pieces from the Original Cube Data-----\n",
            "(5402, 9, 9, 15)\n",
            "training on  cuda\n",
            "epoch 1, train loss 0.047536, train acc 0.871, valida loss 0.019529, valida acc 0.980, time 5.3 sec\n",
            "epoch 2, train loss 0.257277, train acc 0.971, valida loss 0.003489, valida acc 0.985, time 5.2 sec\n",
            "epoch 3, train loss 0.038372, train acc 0.975, valida loss 0.008485, valida acc 0.984, time 5.2 sec\n",
            "epoch 4, train loss 0.230263, train acc 0.991, valida loss 0.039649, valida acc 0.989, time 5.2 sec\n",
            "epoch 5, train loss 0.014684, train acc 0.993, valida loss 0.004259, valida acc 0.989, time 5.2 sec\n",
            "epoch 6, train loss 0.000259, train acc 0.982, valida loss 0.000549, valida acc 0.985, time 5.2 sec\n",
            "epoch 7, train loss 0.000000, train acc 0.995, valida loss 0.000067, valida acc 0.992, time 5.2 sec\n",
            "epoch 8, train loss 0.000103, train acc 0.997, valida loss 0.000053, valida acc 0.987, time 5.2 sec\n",
            "epoch 9, train loss 0.000199, train acc 0.995, valida loss 0.182637, valida acc 0.992, time 5.2 sec\n",
            "epoch 10, train loss 0.000002, train acc 0.999, valida loss 0.070473, valida acc 0.989, time 5.2 sec\n",
            "epoch 11, train loss 0.014660, train acc 0.990, valida loss 0.069794, valida acc 0.993, time 5.2 sec\n",
            "epoch 12, train loss 0.030182, train acc 0.998, valida loss 0.001589, valida acc 0.992, time 5.2 sec\n",
            "epoch 13, train loss 0.000165, train acc 0.995, valida loss 0.052457, valida acc 0.997, time 5.2 sec\n",
            "epoch 14, train loss 0.000001, train acc 1.000, valida loss 0.000179, valida acc 0.998, time 5.3 sec\n",
            "epoch 15, train loss 0.016304, train acc 0.999, valida loss 0.006109, valida acc 0.988, time 5.2 sec\n",
            "epoch 16, train loss 0.000003, train acc 0.998, valida loss 0.002028, valida acc 0.997, time 5.2 sec\n",
            "epoch 17, train loss 0.000095, train acc 0.999, valida loss 0.000000, valida acc 0.995, time 5.2 sec\n",
            "epoch 18, train loss 0.000003, train acc 0.999, valida loss 0.000037, valida acc 0.997, time 5.2 sec\n",
            "epoch 19, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.997, time 5.2 sec\n",
            "epoch 20, train loss 0.001233, train acc 1.000, valida loss 0.000082, valida acc 0.997, time 5.2 sec\n",
            "epoch 21, train loss 0.000005, train acc 0.999, valida loss 0.113598, valida acc 0.980, time 5.2 sec\n",
            "epoch 22, train loss 0.005189, train acc 0.998, valida loss 0.007593, valida acc 0.980, time 5.2 sec\n",
            "epoch 23, train loss 0.000000, train acc 0.992, valida loss 0.000000, valida acc 0.996, time 5.2 sec\n",
            "epoch 24, train loss 0.000000, train acc 0.999, valida loss 0.000013, valida acc 0.996, time 5.2 sec\n",
            "epoch 25, train loss 0.000036, train acc 0.999, valida loss 0.000000, valida acc 0.997, time 5.2 sec\n",
            "epoch 26, train loss 0.000000, train acc 1.000, valida loss 0.000001, valida acc 0.997, time 5.2 sec\n",
            "epoch 27, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.997, time 5.2 sec\n",
            "epoch 28, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.997, time 5.2 sec\n",
            "epoch 29, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.997, time 5.2 sec\n",
            "epoch 30, train loss 0.000061, train acc 0.997, valida loss 0.000035, valida acc 0.992, time 5.2 sec\n",
            "epoch 31, train loss 0.000294, train acc 0.995, valida loss 0.002859, valida acc 0.992, time 5.2 sec\n",
            "epoch 32, train loss 0.000141, train acc 0.998, valida loss 0.000124, valida acc 0.996, time 5.2 sec\n",
            "epoch 33, train loss 0.000004, train acc 0.999, valida loss 1.028077, valida acc 0.997, time 5.2 sec\n",
            "epoch 34, train loss 0.000000, train acc 1.000, valida loss 0.000001, valida acc 0.998, time 5.2 sec\n",
            "epoch 35, train loss 0.000155, train acc 0.999, valida loss 0.000004, valida acc 0.997, time 5.2 sec\n",
            "epoch 36, train loss 0.000000, train acc 1.000, valida loss 0.000003, valida acc 0.997, time 5.2 sec\n",
            "epoch 37, train loss 0.000001, train acc 1.000, valida loss 0.007783, valida acc 0.997, time 5.2 sec\n",
            "epoch 38, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 39, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.997, time 5.2 sec\n",
            "epoch 40, train loss 0.349532, train acc 0.997, valida loss 0.004223, valida acc 0.989, time 5.2 sec\n",
            "epoch 41, train loss 0.000057, train acc 0.998, valida loss 0.000147, valida acc 0.998, time 5.2 sec\n",
            "epoch 42, train loss 0.000003, train acc 1.000, valida loss 0.000007, valida acc 0.998, time 5.3 sec\n",
            "epoch 43, train loss 0.000000, train acc 1.000, valida loss 0.000002, valida acc 0.997, time 5.2 sec\n",
            "epoch 44, train loss 0.000001, train acc 1.000, valida loss 0.000001, valida acc 0.997, time 5.2 sec\n",
            "epoch 45, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 46, train loss 0.000000, train acc 1.000, valida loss 0.000004, valida acc 0.998, time 5.2 sec\n",
            "epoch 47, train loss 0.000004, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 48, train loss 0.000006, train acc 1.000, valida loss 0.000038, valida acc 0.998, time 5.2 sec\n",
            "epoch 49, train loss 0.000000, train acc 1.000, valida loss 0.000010, valida acc 0.999, time 5.2 sec\n",
            "epoch 50, train loss 0.000000, train acc 1.000, valida loss 0.205699, valida acc 0.969, time 5.2 sec\n",
            "epoch 51, train loss 0.000809, train acc 0.994, valida loss 0.015373, valida acc 0.996, time 5.2 sec\n",
            "epoch 52, train loss 0.000000, train acc 0.999, valida loss 0.000278, valida acc 0.998, time 5.2 sec\n",
            "epoch 53, train loss 0.000005, train acc 1.000, valida loss 0.001805, valida acc 0.998, time 5.2 sec\n",
            "epoch 54, train loss 0.000000, train acc 0.999, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 55, train loss 0.000087, train acc 1.000, valida loss 0.000000, valida acc 0.997, time 5.2 sec\n",
            "epoch 56, train loss 0.000000, train acc 1.000, valida loss 0.004322, valida acc 0.998, time 5.2 sec\n",
            "epoch 57, train loss 0.000010, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 58, train loss 0.000000, train acc 0.998, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 59, train loss 0.000003, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 60, train loss 0.000000, train acc 1.000, valida loss 0.000001, valida acc 0.999, time 5.2 sec\n",
            "epoch 61, train loss 0.000012, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 62, train loss 0.000004, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 63, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 64, train loss 0.000027, train acc 0.999, valida loss 0.000000, valida acc 0.994, time 5.2 sec\n",
            "epoch 65, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 66, train loss 0.000000, train acc 1.000, valida loss 0.013703, valida acc 0.999, time 5.2 sec\n",
            "epoch 67, train loss 0.000000, train acc 0.999, valida loss 0.000023, valida acc 0.998, time 5.2 sec\n",
            "epoch 68, train loss 0.000000, train acc 1.000, valida loss 0.000045, valida acc 0.998, time 5.2 sec\n",
            "epoch 69, train loss 0.000000, train acc 1.000, valida loss 0.000001, valida acc 0.999, time 5.2 sec\n",
            "epoch 70, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 71, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.3 sec\n",
            "epoch 72, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 73, train loss 0.000000, train acc 0.999, valida loss 0.000001, valida acc 0.996, time 5.2 sec\n",
            "epoch 74, train loss 0.000089, train acc 0.998, valida loss 0.000001, valida acc 0.996, time 5.2 sec\n",
            "epoch 75, train loss 0.000002, train acc 1.000, valida loss 0.000359, valida acc 0.996, time 5.2 sec\n",
            "epoch 76, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.997, time 5.2 sec\n",
            "epoch 77, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 78, train loss 0.000001, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 79, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 80, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.997, time 5.2 sec\n",
            "epoch 81, train loss 0.000003, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 82, train loss 0.000001, train acc 1.000, valida loss 0.000007, valida acc 0.998, time 5.2 sec\n",
            "epoch 83, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 84, train loss 0.000000, train acc 0.999, valida loss 0.000053, valida acc 0.998, time 5.2 sec\n",
            "epoch 85, train loss 0.000000, train acc 1.000, valida loss 0.000290, valida acc 0.997, time 5.2 sec\n",
            "epoch 86, train loss 0.000000, train acc 0.999, valida loss 0.000000, valida acc 0.997, time 5.2 sec\n",
            "epoch 87, train loss 0.000009, train acc 1.000, valida loss 0.000000, valida acc 0.997, time 5.2 sec\n",
            "epoch 88, train loss 0.000000, train acc 1.000, valida loss 0.621373, valida acc 0.998, time 5.2 sec\n",
            "epoch 89, train loss 0.000000, train acc 1.000, valida loss 0.000007, valida acc 0.998, time 5.2 sec\n",
            "epoch 90, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 91, train loss 0.000001, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 92, train loss 0.000000, train acc 1.000, valida loss 0.000391, valida acc 0.998, time 5.2 sec\n",
            "epoch 93, train loss 0.000000, train acc 1.000, valida loss 0.000005, valida acc 0.998, time 5.2 sec\n",
            "epoch 94, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 95, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 96, train loss 0.000000, train acc 1.000, valida loss 0.000001, valida acc 0.998, time 5.2 sec\n",
            "epoch 97, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 98, train loss 0.000000, train acc 1.000, valida loss 0.000001, valida acc 0.997, time 5.2 sec\n",
            "epoch 99, train loss 0.000000, train acc 1.000, valida loss 0.000001, valida acc 0.997, time 5.2 sec\n",
            "epoch 100, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.3 sec\n",
            "epoch 101, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.995, time 5.2 sec\n",
            "epoch 102, train loss 0.000000, train acc 0.999, valida loss 0.000000, valida acc 0.994, time 5.2 sec\n",
            "epoch 103, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 104, train loss 0.000000, train acc 0.999, valida loss 0.000000, valida acc 0.994, time 5.2 sec\n",
            "epoch 105, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 106, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 107, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 108, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 109, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 110, train loss 0.000005, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 111, train loss 0.000002, train acc 1.000, valida loss 0.189630, valida acc 0.998, time 5.2 sec\n",
            "epoch 112, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 113, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 114, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 115, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 116, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 117, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 118, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 119, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 120, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 121, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 122, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 123, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 124, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 125, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 126, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 127, train loss 0.000000, train acc 1.000, valida loss 0.000017, valida acc 0.998, time 5.2 sec\n",
            "epoch 128, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.3 sec\n",
            "epoch 129, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.3 sec\n",
            "epoch 130, train loss 0.000000, train acc 1.000, valida loss 0.000001, valida acc 0.998, time 5.2 sec\n",
            "epoch 131, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 132, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 133, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 134, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 135, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 136, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 137, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 138, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 139, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.999, time 5.2 sec\n",
            "epoch 140, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.997, time 5.2 sec\n",
            "epoch 141, train loss 0.000000, train acc 1.000, valida loss 0.003170, valida acc 0.998, time 5.2 sec\n",
            "epoch 142, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 143, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 144, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 145, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 146, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 147, train loss 0.000000, train acc 1.000, valida loss 0.133507, valida acc 0.998, time 5.2 sec\n",
            "epoch 148, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 149, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 150, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 151, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 152, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 153, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 154, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 155, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 156, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 157, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.3 sec\n",
            "epoch 158, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 159, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 160, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 161, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 162, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 163, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 164, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 165, train loss 0.000000, train acc 1.000, valida loss 0.096708, valida acc 0.998, time 5.2 sec\n",
            "epoch 166, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 167, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 168, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 169, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 170, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 171, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 172, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 173, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 174, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 175, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 176, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 177, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 178, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 179, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 180, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 181, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 182, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 183, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 184, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 185, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 186, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.3 sec\n",
            "epoch 187, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.3 sec\n",
            "epoch 188, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 189, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 190, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 191, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 192, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 193, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 194, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 195, train loss 0.000000, train acc 1.000, valida loss 0.000001, valida acc 0.998, time 5.2 sec\n",
            "epoch 196, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 197, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 198, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 199, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 200, train loss 0.000000, train acc 1.000, valida loss 0.000000, valida acc 0.998, time 5.2 sec\n",
            "epoch 200, loss 0.0000, train acc 1.000, time 1042.0 sec\n",
            "-------- Training Finished-----------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-846366157324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m     'split' + str(VALIDATION_SPLIT) + 'lr' + str(lr) + PARAM_OPTIM + '.txt')\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m utils.generate_png(\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mall_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_hsi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m'./classification_maps/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'ContextualNetpatch:'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_rows\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mgenerate_png\u001b[0;34m(all_iter, net, gt_hsi, Dataset, device, total_indices, path)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mpred_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt_hsi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mx_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-c48791e4f87a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-Lm-MfHkFwE0"
      }
    }
  ]
}